{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binary-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\FML\\\\fml-project'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NÃ¶tig um eigene Module richtig zu laden\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\admin\\\\Desktop\\\\FML\\\\fml-project\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "major-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, mem_size, input_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "    \n",
    "    #state_ is next state\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1-int(done)\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        states_ =  self.new_state_memory[batch]\n",
    "        actions =  self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, states_, terminal\n",
    "\n",
    "# change this later   \n",
    "def build_dqn(lr, n_actions, input_dims):\n",
    "    inputs = tf.keras.Input(shape=input_dims)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(n_actions, activation=None)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size,\n",
    "                input_dims, epsilon_dec=1e-3, epsilon_end=0.01,\n",
    "                mem_size=1000000, fname='dqn_model.h5'):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = epsilon_dec\n",
    "        self.eps_min = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file = fname\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "        self.q_eval=build_dqn(lr, n_actions, input_dims)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            actions = self.q_eval.predict(observation)\n",
    "            action = np.argmax(actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            # only learn if buffer is full enough\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        q_eval = self.q_eval.predict(states)\n",
    "        q_next = self.q_eval.predict(states_)\n",
    "\n",
    "        q_target = np.copy(q_eval)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * np.max(q_next, axis=1)*dones\n",
    "\n",
    "        self.q_eval.train_on_batch(states, q_target)\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, score: [-2.0001], avg_score: -2.0000998973846436, epsilon: 1.0, num_turns: 8\n",
      "episode: 1, score: [-2.], avg_score: -2.0000500679016113, epsilon: 1.0, num_turns: 9\n",
      "episode: 2, score: [-1.9997001], avg_score: -1.9999333620071411, epsilon: 1.0, num_turns: 9\n",
      "episode: 3, score: [-1.9994], avg_score: -1.9998000860214233, epsilon: 1.0, num_turns: 6\n",
      "episode: 4, score: [-2.0001], avg_score: -1.9998600482940674, epsilon: 1.0, num_turns: 14\n",
      "episode: 5, score: [-1.9995], avg_score: -1.9998000860214233, epsilon: 1.0, num_turns: 5\n",
      "episode: 6, score: [-1.89], avg_score: -1.984114408493042, epsilon: 1.0, num_turns: 9\n",
      "episode: 7, score: [-1.8003], avg_score: -1.9611375331878662, epsilon: 0.991, num_turns: 12\n",
      "episode: 8, score: [-1.8897], avg_score: -1.9531999826431274, epsilon: 0.985, num_turns: 6\n",
      "episode: 9, score: [-1.9997001], avg_score: -1.9578500986099243, epsilon: 0.979, num_turns: 6\n",
      "episode: 10, score: [-1.9997001], avg_score: -1.9616546630859375, epsilon: 0.973, num_turns: 6\n",
      "episode: 11, score: [-0.9999001], avg_score: -1.881508469581604, epsilon: 0.96, num_turns: 13\n",
      "episode: 12, score: [-1.889], avg_score: -1.8820847272872925, epsilon: 0.944, num_turns: 16\n",
      "episode: 13, score: [-1.9996], avg_score: -1.8904787302017212, epsilon: 0.9369999999999999, num_turns: 7\n",
      "episode: 14, score: [-1.8998001], avg_score: -1.8911000490188599, epsilon: 0.9319999999999999, num_turns: 5\n",
      "episode: 15, score: [-1.8896999], avg_score: -1.8910125494003296, epsilon: 0.9199999999999999, num_turns: 12\n",
      "episode: 16, score: [-1.8997], avg_score: -1.8915234804153442, epsilon: 0.9139999999999999, num_turns: 6\n",
      "episode: 17, score: [-1.9991], avg_score: -1.897499918937683, epsilon: 0.9079999999999999, num_turns: 6\n",
      "episode: 18, score: [-1.9995], avg_score: -1.902868390083313, epsilon: 0.9029999999999999, num_turns: 5\n",
      "episode: 19, score: [-2.], avg_score: -1.9077249765396118, epsilon: 0.8909999999999999, num_turns: 12\n",
      "episode: 20, score: [-1.7895], avg_score: -1.9020951986312866, epsilon: 0.8859999999999999, num_turns: 5\n",
      "episode: 21, score: [-2.0001001], avg_score: -1.9065499305725098, epsilon: 0.8689999999999999, num_turns: 17\n",
      "episode: 22, score: [-1.9995], avg_score: -1.9105912446975708, epsilon: 0.8639999999999999, num_turns: 5\n",
      "episode: 23, score: [-2.], avg_score: -1.9143167734146118, epsilon: 0.8579999999999999, num_turns: 6\n",
      "episode: 24, score: [-1.9996], avg_score: -1.917728066444397, epsilon: 0.8509999999999999, num_turns: 7\n",
      "episode: 25, score: [-1.9995], avg_score: -1.9208731651306152, epsilon: 0.8459999999999999, num_turns: 5\n",
      "episode: 26, score: [-1.9991], avg_score: -1.9237704277038574, epsilon: 0.8339999999999999, num_turns: 12\n",
      "episode: 27, score: [-2.0002], avg_score: -1.9265000820159912, epsilon: 0.8239999999999998, num_turns: 10\n",
      "episode: 28, score: [-1.9989], avg_score: -1.9289966821670532, epsilon: 0.8129999999999998, num_turns: 11\n",
      "episode: 29, score: [-1.9995], avg_score: -1.9313467741012573, epsilon: 0.8049999999999998, num_turns: 8\n",
      "episode: 30, score: [-1.8896], avg_score: -1.9300000667572021, epsilon: 0.7979999999999998, num_turns: 7\n",
      "episode: 31, score: [-1.8996], avg_score: -1.9290499687194824, epsilon: 0.7909999999999998, num_turns: 7\n",
      "episode: 32, score: [-1.9993], avg_score: -1.9311786890029907, epsilon: 0.7719999999999998, num_turns: 19\n",
      "episode: 33, score: [-1.9995], avg_score: -1.9331880807876587, epsilon: 0.7669999999999998, num_turns: 5\n",
      "episode: 34, score: [-1.9995], avg_score: -1.9350825548171997, epsilon: 0.7619999999999998, num_turns: 5\n",
      "episode: 35, score: [-1.9997001], avg_score: -1.9368776082992554, epsilon: 0.7559999999999998, num_turns: 6\n",
      "episode: 36, score: [-1.8997], avg_score: -1.9358726739883423, epsilon: 0.7499999999999998, num_turns: 6\n",
      "episode: 37, score: [-2.0005], avg_score: -1.9375734329223633, epsilon: 0.7369999999999998, num_turns: 13\n",
      "episode: 38, score: [-1.9998001], avg_score: -1.9391690492630005, epsilon: 0.7319999999999998, num_turns: 5\n",
      "episode: 39, score: [-1.9995], avg_score: -1.9406776428222656, epsilon: 0.7269999999999998, num_turns: 5\n",
      "episode: 40, score: [-1.9995], avg_score: -1.9421122074127197, epsilon: 0.7219999999999998, num_turns: 5\n",
      "episode: 41, score: [-1.7995], avg_score: -1.9387166500091553, epsilon: 0.7169999999999997, num_turns: 5\n",
      "episode: 42, score: [-1.8898001], avg_score: -1.9375791549682617, epsilon: 0.7119999999999997, num_turns: 5\n",
      "episode: 43, score: [-1.9995], avg_score: -1.9389863014221191, epsilon: 0.7069999999999997, num_turns: 5\n",
      "episode: 44, score: [-2.], avg_score: -1.9403421878814697, epsilon: 0.6979999999999997, num_turns: 9\n",
      "episode: 45, score: [-1.8903], avg_score: -1.9392542839050293, epsilon: 0.6889999999999997, num_turns: 9\n",
      "episode: 46, score: [-2.], avg_score: -1.9405467510223389, epsilon: 0.6829999999999997, num_turns: 6\n",
      "episode: 47, score: [-1.9997001], avg_score: -1.9417791366577148, epsilon: 0.6769999999999997, num_turns: 6\n",
      "episode: 48, score: [-1.8997], avg_score: -1.940920352935791, epsilon: 0.6709999999999997, num_turns: 6\n",
      "episode: 49, score: [-1.8901], avg_score: -1.9399038553237915, epsilon: 0.6629999999999997, num_turns: 8\n",
      "episode: 50, score: [-1.9996], avg_score: -1.9410744905471802, epsilon: 0.6559999999999997, num_turns: 7\n",
      "episode: 51, score: [-0.99970007], avg_score: -1.9229711294174194, epsilon: 0.6499999999999997, num_turns: 6\n",
      "episode: 52, score: [-1.9995999], avg_score: -1.9244170188903809, epsilon: 0.6369999999999997, num_turns: 13\n",
      "episode: 53, score: [-1.9997001], avg_score: -1.9258111715316772, epsilon: 0.6309999999999997, num_turns: 6\n",
      "episode: 54, score: [-1.8895], avg_score: -1.9251511096954346, epsilon: 0.6259999999999997, num_turns: 5\n",
      "episode: 55, score: [-1.8995], avg_score: -1.924692988395691, epsilon: 0.6209999999999997, num_turns: 5\n",
      "episode: 56, score: [-1.7895], avg_score: -1.922321081161499, epsilon: 0.6159999999999997, num_turns: 5\n",
      "episode: 57, score: [-1.9997001], avg_score: -1.9236552715301514, epsilon: 0.6099999999999997, num_turns: 6\n",
      "episode: 58, score: [-1.9988999], avg_score: -1.9249306917190552, epsilon: 0.5869999999999996, num_turns: 23\n",
      "episode: 59, score: [-1.9997001], avg_score: -1.9261767864227295, epsilon: 0.5809999999999996, num_turns: 6\n",
      "episode: 60, score: [-1.9995], avg_score: -1.92737877368927, epsilon: 0.5759999999999996, num_turns: 5\n",
      "episode: 61, score: [-2.0001001], avg_score: -1.9285516738891602, epsilon: 0.5679999999999996, num_turns: 8\n",
      "episode: 62, score: [-2.0004], avg_score: -1.9296921491622925, epsilon: 0.5569999999999996, num_turns: 11\n",
      "episode: 63, score: [-1.7891], avg_score: -1.9274954795837402, epsilon: 0.5419999999999996, num_turns: 15\n",
      "episode: 64, score: [-1.9995], avg_score: -1.928603172302246, epsilon: 0.5369999999999996, num_turns: 5\n",
      "episode: 65, score: [-1.6995], avg_score: -1.925131916999817, epsilon: 0.5319999999999996, num_turns: 5\n",
      "episode: 66, score: [-1.9996], avg_score: -1.9262433052062988, epsilon: 0.5249999999999996, num_turns: 7\n",
      "episode: 67, score: [-1.8991], avg_score: -1.9258440732955933, epsilon: 0.5159999999999996, num_turns: 9\n",
      "episode: 68, score: [-1.9997001], avg_score: -1.9269143342971802, epsilon: 0.5099999999999996, num_turns: 6\n",
      "episode: 69, score: [-1.8995], avg_score: -1.9265228509902954, epsilon: 0.5049999999999996, num_turns: 5\n",
      "episode: 70, score: [-1.9994], avg_score: -1.9275493621826172, epsilon: 0.49899999999999956, num_turns: 6\n",
      "episode: 71, score: [-1.9994], avg_score: -1.9285473823547363, epsilon: 0.49299999999999955, num_turns: 6\n",
      "episode: 72, score: [-1.9992], avg_score: -1.9295152425765991, epsilon: 0.48799999999999955, num_turns: 5\n",
      "episode: 73, score: [-1.8898001], avg_score: -1.9289785623550415, epsilon: 0.48299999999999954, num_turns: 5\n",
      "episode: 74, score: [-1.8898001], avg_score: -1.92845618724823, epsilon: 0.47799999999999954, num_turns: 5\n",
      "episode: 75, score: [-1.8994], avg_score: -1.9280738830566406, epsilon: 0.47199999999999953, num_turns: 6\n",
      "episode: 76, score: [-1.8895], avg_score: -1.9275728464126587, epsilon: 0.4669999999999995, num_turns: 5\n",
      "episode: 77, score: [-1.9991001], avg_score: -1.9284899234771729, epsilon: 0.4549999999999995, num_turns: 12\n",
      "episode: 78, score: [-1.9993], avg_score: -1.9293861389160156, epsilon: 0.4479999999999995, num_turns: 7\n",
      "episode: 79, score: [-1.9998001], avg_score: -1.9302663803100586, epsilon: 0.4429999999999995, num_turns: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 80, score: [-1.9998001], avg_score: -1.9311248064041138, epsilon: 0.4379999999999995, num_turns: 5\n",
      "episode: 81, score: [-1.9992], avg_score: -1.931955099105835, epsilon: 0.4329999999999995, num_turns: 5\n",
      "episode: 82, score: [-1.9997001], avg_score: -1.9327712059020996, epsilon: 0.4269999999999995, num_turns: 6\n",
      "episode: 83, score: [-1.9998001], avg_score: -1.9335691928863525, epsilon: 0.4219999999999995, num_turns: 5\n",
      "episode: 84, score: [-1.9995], avg_score: -1.9343448877334595, epsilon: 0.4169999999999995, num_turns: 5\n",
      "episode: 85, score: [-1.8897], avg_score: -1.9338256120681763, epsilon: 0.4109999999999995, num_turns: 6\n",
      "episode: 86, score: [-1.8995], avg_score: -1.9334311485290527, epsilon: 0.4059999999999995, num_turns: 5\n",
      "episode: 87, score: [-1.9995], avg_score: -1.9341819286346436, epsilon: 0.40099999999999947, num_turns: 5\n",
      "episode: 88, score: [-1.9996], avg_score: -1.9349169731140137, epsilon: 0.39099999999999946, num_turns: 10\n",
      "episode: 89, score: [-1.9984001], avg_score: -1.9356223344802856, epsilon: 0.38099999999999945, num_turns: 10\n",
      "episode: 90, score: [-1.8895999], avg_score: -1.9351166486740112, epsilon: 0.37399999999999944, num_turns: 7\n",
      "episode: 91, score: [-1.8994], avg_score: -1.9347283840179443, epsilon: 0.36799999999999944, num_turns: 6\n",
      "episode: 92, score: [-1.6885], avg_score: -1.932080864906311, epsilon: 0.35599999999999943, num_turns: 12\n",
      "episode: 93, score: [-1.9995], avg_score: -1.9327980279922485, epsilon: 0.3509999999999994, num_turns: 5\n",
      "episode: 94, score: [-2.], avg_score: -1.9335054159164429, epsilon: 0.3449999999999994, num_turns: 6\n",
      "episode: 95, score: [-1.8995], avg_score: -1.9331512451171875, epsilon: 0.3399999999999994, num_turns: 5\n",
      "episode: 96, score: [-1.8897], avg_score: -1.9327032566070557, epsilon: 0.3339999999999994, num_turns: 6\n",
      "episode: 97, score: [-1.9998001], avg_score: -1.9333878755569458, epsilon: 0.3289999999999994, num_turns: 5\n",
      "episode: 98, score: [-1.9994], avg_score: -1.9340547323226929, epsilon: 0.3229999999999994, num_turns: 6\n",
      "episode: 99, score: [-1.7895], avg_score: -1.9326092004776, epsilon: 0.3179999999999994, num_turns: 5\n",
      "episode: 100, score: [-1.9998001], avg_score: -1.9326062202453613, epsilon: 0.3129999999999994, num_turns: 5\n",
      "episode: 101, score: [-1.9996], avg_score: -1.9326022863388062, epsilon: 0.3059999999999994, num_turns: 7\n",
      "episode: 102, score: [-2.], avg_score: -1.9326051473617554, epsilon: 0.2999999999999994, num_turns: 6\n",
      "episode: 103, score: [-2.0001001], avg_score: -1.9326121807098389, epsilon: 0.29199999999999937, num_turns: 8\n",
      "episode: 104, score: [-1.9999], avg_score: -1.932610034942627, epsilon: 0.27899999999999936, num_turns: 13\n",
      "episode: 105, score: [-1.9993], avg_score: -1.9326080083847046, epsilon: 0.27199999999999935, num_turns: 7\n",
      "episode: 106, score: [-1.9992], avg_score: -1.9336999654769897, epsilon: 0.26699999999999935, num_turns: 5\n",
      "episode: 107, score: [-1.9995], avg_score: -1.9356919527053833, epsilon: 0.26199999999999934, num_turns: 5\n",
      "episode: 108, score: [-1.9997001], avg_score: -1.936792016029358, epsilon: 0.25599999999999934, num_turns: 6\n",
      "episode: 109, score: [-1.8883001], avg_score: -1.9356781244277954, epsilon: 0.24499999999999933, num_turns: 11\n",
      "episode: 110, score: [-1.9998001], avg_score: -1.9356789588928223, epsilon: 0.23999999999999932, num_turns: 5\n",
      "episode: 111, score: [-1.0004998], avg_score: -1.935685157775879, epsilon: 0.22999999999999932, num_turns: 10\n",
      "episode: 112, score: [-1.9994], avg_score: -1.9367892742156982, epsilon: 0.2239999999999993, num_turns: 6\n",
      "episode: 113, score: [-1.8895], avg_score: -1.9356881380081177, epsilon: 0.2189999999999993, num_turns: 5\n",
      "episode: 114, score: [-1.9995], avg_score: -1.9366850852966309, epsilon: 0.2139999999999993, num_turns: 5\n",
      "episode: 115, score: [-1.8898001], avg_score: -1.9366860389709473, epsilon: 0.2089999999999993, num_turns: 5\n",
      "episode: 116, score: [-1.7898], avg_score: -1.9355870485305786, epsilon: 0.2009999999999993, num_turns: 8\n",
      "episode: 117, score: [-1.9992], avg_score: -1.9355881214141846, epsilon: 0.19599999999999929, num_turns: 5\n",
      "episode: 118, score: [-1.9998001], avg_score: -1.9355909824371338, epsilon: 0.19099999999999928, num_turns: 5\n",
      "episode: 119, score: [-1.9995], avg_score: -1.9355859756469727, epsilon: 0.18599999999999928, num_turns: 5\n",
      "episode: 120, score: [-1.9995], avg_score: -1.9376859664916992, epsilon: 0.18099999999999927, num_turns: 5\n",
      "episode: 121, score: [-1.9995], avg_score: -1.9376800060272217, epsilon: 0.17599999999999927, num_turns: 5\n",
      "episode: 122, score: [-1.9995], avg_score: -1.9376798868179321, epsilon: 0.17099999999999926, num_turns: 5\n",
      "episode: 123, score: [-1.9995], avg_score: -1.937674880027771, epsilon: 0.16599999999999926, num_turns: 5\n",
      "episode: 124, score: [-1.9992], avg_score: -1.9376710653305054, epsilon: 0.16099999999999925, num_turns: 5\n",
      "episode: 125, score: [-1.9997001], avg_score: -1.9376730918884277, epsilon: 0.15499999999999925, num_turns: 6\n",
      "episode: 126, score: [-1.8895], avg_score: -1.9365769624710083, epsilon: 0.14999999999999925, num_turns: 5\n",
      "episode: 127, score: [-1.9013], avg_score: -1.9355881214141846, epsilon: 0.13299999999999923, num_turns: 17\n",
      "episode: 128, score: [-1.7895], avg_score: -1.9334940910339355, epsilon: 0.12799999999999923, num_turns: 5\n",
      "episode: 129, score: [-1.8895], avg_score: -1.9323941469192505, epsilon: 0.12299999999999922, num_turns: 5\n",
      "episode: 130, score: [-2.0015001], avg_score: -1.9335130453109741, epsilon: 0.10799999999999921, num_turns: 15\n",
      "episode: 131, score: [-1.9998001], avg_score: -1.934515118598938, epsilon: 0.1029999999999992, num_turns: 5\n",
      "episode: 132, score: [-1.9992], avg_score: -1.9345141649246216, epsilon: 0.0979999999999992, num_turns: 5\n",
      "episode: 133, score: [-1.9998001], avg_score: -1.9345170259475708, epsilon: 0.0929999999999992, num_turns: 5\n",
      "episode: 134, score: [-1.9998001], avg_score: -1.9345201253890991, epsilon: 0.08799999999999919, num_turns: 5\n",
      "episode: 135, score: [-1.9989], avg_score: -1.9345120191574097, epsilon: 0.07699999999999918, num_turns: 11\n",
      "episode: 136, score: [-2.0011], avg_score: -1.9355261325836182, epsilon: 0.05499999999999916, num_turns: 22\n",
      "episode: 137, score: [-1.9996], avg_score: -1.9355170726776123, epsilon: 0.047999999999999154, num_turns: 7\n",
      "episode: 138, score: [-1.8904], avg_score: -1.9344230890274048, epsilon: 0.036999999999999145, num_turns: 11\n",
      "episode: 139, score: [-2.], avg_score: -1.9344279766082764, epsilon: 0.03099999999999914, num_turns: 6\n",
      "episode: 140, score: [-1.9994], avg_score: -1.9344271421432495, epsilon: 0.024999999999999134, num_turns: 6\n",
      "episode: 141, score: [-1.9995], avg_score: -1.936427116394043, epsilon: 0.01999999999999913, num_turns: 5\n",
      "episode: 142, score: [-1.9998001], avg_score: -1.937527060508728, epsilon: 0.014999999999999125, num_turns: 5\n",
      "episode: 143, score: [-1.8986], avg_score: -1.9365181922912598, epsilon: 0.00999999999999912, num_turns: 5\n",
      "episode: 144, score: [-1.9998001], avg_score: -1.9365161657333374, epsilon: 0.01, num_turns: 5\n",
      "episode: 145, score: [-1.8898001], avg_score: -1.9365110397338867, epsilon: 0.01, num_turns: 5\n",
      "episode: 146, score: [-1.8898001], avg_score: -1.9354091882705688, epsilon: 0.01, num_turns: 5\n",
      "episode: 147, score: [-1.9995], avg_score: -1.9354071617126465, epsilon: 0.01, num_turns: 5\n",
      "episode: 148, score: [-1.9997001], avg_score: -1.9364068508148193, epsilon: 0.01, num_turns: 6\n",
      "episode: 149, score: [-1.9994], avg_score: -1.9375, epsilon: 0.01, num_turns: 6\n",
      "episode: 150, score: [-1.9998001], avg_score: -1.9375020265579224, epsilon: 0.01, num_turns: 5\n",
      "episode: 151, score: [-1.9997001], avg_score: -1.9475017786026, epsilon: 0.01, num_turns: 6\n",
      "episode: 152, score: [-1.9995], avg_score: -1.9475009441375732, epsilon: 0.01, num_turns: 5\n",
      "episode: 153, score: [-1.9995], avg_score: -1.9474987983703613, epsilon: 0.01, num_turns: 5\n",
      "episode: 154, score: [-1.9997001], avg_score: -1.9486007690429688, epsilon: 0.01, num_turns: 6\n",
      "episode: 155, score: [-1.9995], avg_score: -1.9496006965637207, epsilon: 0.01, num_turns: 5\n",
      "episode: 156, score: [-1.8995], avg_score: -1.9507008790969849, epsilon: 0.01, num_turns: 5\n",
      "episode: 157, score: [-1.9992], avg_score: -1.9506959915161133, epsilon: 0.01, num_turns: 5\n",
      "episode: 158, score: [-1.7895], avg_score: -1.9486021995544434, epsilon: 0.01, num_turns: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 159, score: [-1.7897], avg_score: -1.9465022087097168, epsilon: 0.01, num_turns: 6\n",
      "episode: 160, score: [-1.8998001], avg_score: -1.9455050230026245, epsilon: 0.01, num_turns: 5\n",
      "episode: 161, score: [-1.7997], avg_score: -1.9435009956359863, epsilon: 0.01, num_turns: 6\n",
      "episode: 162, score: [-1.7898], avg_score: -1.9413949251174927, epsilon: 0.01, num_turns: 5\n",
      "episode: 163, score: [-1.903], avg_score: -1.9425339698791504, epsilon: 0.01, num_turns: 24\n",
      "episode: 164, score: [-1.9998001], avg_score: -1.9425369501113892, epsilon: 0.01, num_turns: 5\n",
      "episode: 165, score: [-1.9997001], avg_score: -1.9455389976501465, epsilon: 0.01, num_turns: 6\n",
      "episode: 166, score: [-1.9995], avg_score: -1.94553804397583, epsilon: 0.01, num_turns: 5\n",
      "episode: 167, score: [-1.9996], avg_score: -1.9465429782867432, epsilon: 0.01, num_turns: 7\n"
     ]
    }
   ],
   "source": [
    "from adapter.bomberman_adapter import BombermanEnvironment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "env = tf_py_environment.TFPyEnvironment(BombermanEnvironment())\n",
    "lr = 0.001\n",
    "n_games = 500\n",
    "n_actions = 6 #env.step expects an int between 0 and 5\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, lr=lr,\n",
    "              input_dims=env.observation_spec().shape,\n",
    "             n_actions=n_actions, mem_size=100000, batch_size=64,\n",
    "             epsilon_end=0.01)\n",
    "scores = []\n",
    "eps_history=[]\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    timestep = env.reset()\n",
    "    observation = timestep.observation\n",
    "    turn=0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        timestep = env.step(action)\n",
    "        observation_ = timestep.observation\n",
    "        reward = timestep.reward.numpy()\n",
    "        done = True if timestep.step_type.numpy()[0]==2 else False\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "        turn +=1\n",
    "        \n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(f'episode: {i}, score: {score}, avg_score: {avg_score}, epsilon: {agent.epsilon}, num_turns: {turn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-absolute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
